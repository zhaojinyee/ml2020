{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd704f-e529-4420-8f27-a5554388344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#先import需要的库函数\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from touch.utils.data import DataLoader,Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03add6-80d2-4733-9745-e8f590d7ef63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#定义读取图片文件函数\n",
    "def readfile(path,flag):\n",
    "    \"\"\"\n",
    "    path:图片所在文件夹的路径\n",
    "    flag:为True时是training set或validation set,为False时是testing set\n",
    "    return值为数值化后的图片数据\n",
    "    \"\"\"\n",
    "    image_dir=os.listdir(path) #将path路径指定的文件夹内的所有文件路径以列表形式返回\n",
    "    #先创建两个规定大小的全零array\n",
    "    x=np.zeros((len(image_dir),128,128,3) #3为通道数\n",
    "    y=np.zeros((len(image_dir))) #x为所有图片叠加成的一个矩阵，y则为所有图片的标签形成的矩阵\n",
    "    for i,file in enumerate(image_dir): #遍历每一张图片，并将x，y内元素赋值，enumerate用于将一个可遍历组合为一个索引序列\n",
    "        img=cv2.imread(os.path.join(path,file)) \n",
    "        #os.path.join()用于将两个路径拼接，如此出path是图片文件夹路径，file就是图片名，组合起来就是每张图片路径\n",
    "        x[i,:,:]=cv2.resize(img,(128,128)) #图片赋值到x中前，由于图片size不同，都变成128*128的\n",
    "        if flag: #training或alidation\n",
    "            y[i]=int(file.split('_')[0]) #截取图片名中种类的那部分\n",
    "    if flag: #training和validation\n",
    "        return x,y\n",
    "    else: #testing\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd90241-1be8-4d98-b1d5-bdaf6f9f745d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#用上面readfile函数把training set,validation set,testing set读进来\n",
    "workspace_dir='/home/zjy/ml hw3/food-11' #用于后面路径拼接\n",
    "print(\"Reading data\")\n",
    "train_x,train_y=readfile(os.path.join(workpace_dir,\"training\"),True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x,val_y=readfile(os.path.join(workpace_dir,\"validation\"),True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x=readfile(os.path.join(workpace_dir,\"testing\"),False)\n",
    "print(\"Size of testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22a8b0-3459-4ec0-991a-97b629c0771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对data做augmentation数据增强,即通过翻转旋转图片等来获得更多的training data，同时将格式转换成tenser形式\n",
    "train_transform=transforms.Compose([ #对training data的transform，compose是用于将多个变换打包在一起\n",
    "    transforms.ToPILImage()，#图像转为pil\n",
    "    transforms.RandomHorizontalFlip(), #随机将图像水平翻转\n",
    "    transforms.RandomRotation(15), #随机将图像旋转15度\n",
    "    transforms.ToTensor(), #将图像变成tensor并normalize到[0,1]\n",
    "])\n",
    "#testing data不用data augmentation\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97653e-4e69-4c5a-94ea-ca674bdaac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用Dataset和DataLoader来打包包装data，分成多个batch后再输出，使后续testing更方便\n",
    "Dataset是用来继承的父类，可以实现对数据的封装，继承了Dataset后要重写len和getitem这两个方法\n",
    "len方法是给出dataset的大小，getitem支持索引从0到len(self)的数据\n",
    "DataLoader是用过getitem函数获得单个数据并可以组合成batch\n",
    "\"\"\"\n",
    "class ImagDataset(Dataset): \n",
    "\"\"\"\n",
    "类名开头大写,__init__初始化函数，在定义对象时，对象的实参对应初始化函数的形参\n",
    "self可看成对象，所有类的方法形参第一个都要是self，在写类的属性时也要用self.\n",
    "\"\"\"\n",
    "    def __init__(self,x,y=None,transform=None): \n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        if y is not None: #label要是longtensor形式\n",
    "            self.y=torch.LongTensor(y)\n",
    "        self.transform=transform\n",
    "    def __len__(self)：\n",
    "        return len(self.x)\n",
    "    def __getitem__(self,index):\n",
    "        X=self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X=self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y=self.y[index]\n",
    "            return X,Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826122b-1089-49d6-ba03-389685d0278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来调用上面的定义来进行分批，并用Dataloader完成最后的包装\n",
    "batch_size=64 #设置bach_size\n",
    "train_set=ImgDataset(train_x,train_y,train_transform)\n",
    "val_set=ImgDataset(val_x,val_y,test_transform)\n",
    "train_loader=DataLoader(train_set,batch_size=batch_size，shuffle=True) #shuffle是用于决定每次迭代训练时是否对数据洗牌\n",
    "val_loader=DataLoader(val_set,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177daf6f-5a85-480a-9382-b638bae14bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行网络的搭建\n",
    "\"\"\"\n",
    "构建网络需要继承nn.Module父类，并调用nn.Module的构造函数\n",
    "有两个module，一个是cnn卷积层，一个是fc即fully connected层，fc层是在cnn出来后flatten再进入\n",
    "cnn层要利用nn.Conv2d,nn.BatchNorm2d,nn.ReLU,nn.MaxPool2d这四个函数构建一个五层的cnn\n",
    "BatchNorm2d层一般是跟在Conv2d层后做归一化\n",
    "\"\"\"\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__() #super().__init__()作用就是让我们可以执行父类的构造函数同时调用父类的属性\n",
    "        \"\"\"\n",
    "        以下列出要用函数的形参\n",
    "        torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        torch.nn.MaxPool2d(kernel_size,stride,padding)\n",
    "        以上kernel_size就是window的size，padding时边缘是否填0\n",
    "        \"\"\"\n",
    "        #卷积层\n",
    "        self.nn=nn.Sequential(\n",
    "            #卷积层1\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  #output：[64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #output:[64, 64, 64]\n",
    "            #卷积层2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),  #output：[128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #output：[128, 32, 32]\n",
    "            #卷积层3\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),  #output：[256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #output：[256, 16, 16]\n",
    "            #卷积层4\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),  #output：[512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #output：[512, 8, 8]\n",
    "            #卷积层5\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),  # utput：[512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #output：[512, 4, 4]\n",
    "        )\n",
    "        #fully connected层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 11)\n",
    "        #pytorch和keras不同，keras中flatten函数是和卷积一起写的，但pytorch中是要单独在forward中用view写\n",
    "            def forward(self,x): #forward将上两个网络调用\n",
    "                out=self.cnn(x)\n",
    "                out=out.view(out.size()[0],-1) #flatten\n",
    "                return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e32c8-61eb-496d-9389-74a6d952aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train模型\n",
    "model=Classifier().cuda() #使用gpu计算\n",
    "loss=nn.CrossEntropyLoss() #Loss用crossentropy\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001) #optimizer是自动做gradient descent的优化器，并在此处指定用Adam做\n",
    "num_epoch=30 #迭代次数\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time=time.time()\n",
    "    train_acc=0.0 #计算每个epoch的accuracy和loss\n",
    "    train_loss=0.0\n",
    "    val_acc=0.0\n",
    "    val_loss=0.0\n",
    "    \n",
    "model.train() #告诉现在是在模型训练，让参数可以变化，开启dropout等\n",
    "for i,data in enumerate(train_loader):\n",
    "    optimizer.zero_grad() #每次使用optimizer前都先将前面存的gradient清零\n",
    "    #data[0]=x,data[1]=y\n",
    "    #利用model函数前向传播（forward），算预测值\n",
    "    train_pred=model(data[0].cuda())\n",
    "    batch_loss=loss(train_pred,data[1].cuda())\n",
    "    #用算出来的batch_loss进行反向传播（back propagation）算出每个参数的gradient、\n",
    "    batch_loss.backward()\n",
    "    optimizer.step() #用step来更新参数\n",
    "    #计算最终总的acc和loss\n",
    "    #np.argmax()返回最大值的索引，axis=1则是对行进行，返回的索引正好就对应了标签，然后和y真实标签比较，则可得到分类正确的数量\n",
    "    train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "    train_loss += batch_loss.item() #张量中只有一个元素就可以使用item()方法读取\n",
    "\n",
    "model.eval() #告诉现在是用validation data或testing data评估模型，让算出的参数不变\n",
    "with torch.no_grad():# 进行验证，当gradient不变时\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        # 將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "              (epoch + 1, num_epoch, time.time() - epoch_start_time, \\\n",
    "               train_acc / train_set.__len__(), train_loss / train_set.__len__(), val_acc / val_set.__len__(),\n",
    "               val_loss / val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e524a-6621-43e1-bced-59c125dd3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于精确度不高，将traning data和vlidation data合成一个训练集，再次训练\n",
    "model_best = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5a841-3b59-4fc7-ad5d-4cdc16a652a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在testing data上跑，并将数据写入csv文件\n",
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)\n",
    "\"\"\"\n",
    "#保存预测结果\n",
    "with open(\"predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
